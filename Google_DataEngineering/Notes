	1. Object: Object Strorage
	-> Global namespace
	-> Classes 
		a. High Performance (Frequency)
		b. Backup and Archival (Nearline (1 per month), Coldline (3month) , Archive(1 per year))
	-> Redundancy 
		a. Regional (no auto failover)
		b. Dual Region
		c. Multi-region 
	-> Lifecycle management
		Automatically transition to lower cost stoorage class - specified by lifecycle policy\]
		Delete (If versioned, delete live version create non curent version) (but if non current -> really delted)
		Change Storage Class. Multi regional /Standard -> Nearline->Coldline->Archive (cant reverse)(12months)
		READ: Age, created before, is live, matches storage class, num of newer versions!!!!!!!.
	-> Security
		Autmatic encryption
		Customer managed - using Cloud key
		Customer supplied - using key generated by the customer
		ACL
		Uniform Bucket level access (recommended). Fine grained is outdated
	-> load
		Small -> console, command (gsutil), API
		Gsutil mb (make bucket) gs://name
		Gsutil cp x.txt gs://name
		Gsutil cp -m (= multithreading, faster)
		Gsutil rsync data gs:// (synronize content of directories0 (automatically place successful files (wont delete if ther are fail cases)
		
		Large -> Cloud storage transfer (<5GB,>1TB. Works for AWS S3 One time/recurring transfer), transfer appliance (very big data like 100/480 TB, physical attach to your network)
		
		Faster solution = 1. -m (parallel) 2. Assem0 archive (TAR) file. Transmit the TAR files instead
	-> access 
		a. Uniform bucket level access (recommended) IAM . Applied permissions to object in bucket or group of object with common prefix.
		b. Fine-grained. Legacy access control method. Apply permissions to both bucket and object level
		c. Signed URL - time limited r+w . Allow access without IAM
		d. Signed Policy Doc. Specify what can be uploaded to a bucket. Control Size and type

	2. Relational: Cloud SQL, Cloud Spanner
		a. ACID transactions, Strong consistency, Predefined schema, Partitions shard data, Normalization ->primary key
		A. Cloud SQL 
			i. Postgres,My,SQL
			ii. Regional data / multiple zones for HA/ multi region = backups only
			iii. 30TB only
			iv. Cloud SQL Auth proxy for API
			v. Read replica fo analysis ( readonly)
		B. Cloud Spanner
			i. Globally distributed
			ii. HA
			iii. Up to 2TB per node
			iv. No planned downtime; autmatic replication
			v. Hot Spot occurs when many read and write on the same node.
				1) Happen with sequential primary keys (auto-inc values/ timestamps)
				2) Key Visualizer to determine hotspot!
				3) Consider using hash value of primary; bit-reverse sequential values (not random!!)
				4) Also promote high cardinality attributes
				5) Hot spotting avoid = store datetime in the end of  the rowkey
			vi. Interleaved tables
				1) If there is parent-child relationship
					a) Order to order line, person to address one to many relationship
					b) Row from parent table stored with rows from child table
					c) More efficient when retrieving data from both
					d) "  INTERLEAVE IN PARENT Singers ON DELETE CASCADE;"
	3. NoSQL: Firestore
		A. Key value/ Hierarchical
		B. Two modes - native mode (mobile sync) or datastore mode (web/server based)
		C. Index = Atomic values Descending + Ascending
		D. Semi structured data; may change over time; different entities
		E. Query on multiple attributes
		F. -Bounded ingestion volume
		G. Entity and Kinds
			i. Entity types: Atomic values/ Arrays/Entities
			ii. Indexing: Index required for all quiries
			iii. Kind = A group
			iv.  Cloud Firestore stores data redundantly when multiple indexes are used, so having more indexes will lead to greater storage sizes. 
			v. Transaction = atomic
				1) Serializable Isolation - data read by a transaction cannot be concurrently modified
					a) Consistency
					b) Maximum duration of 60 secs
					c) 10 sec idle expiration after 30secs
					d) Modify up to 500 entities in a single transaction
	4. Wide: BigTable
		A. Relational database build oin table abstraction
		B. Sparse multi-dimensional storage
		C. + Petabyte scale data
		D. Low Latency
		E. Key based read
		F. In order to load Cloud Storage Data to BigTable -> Dataflow. Not gsutil even via Storage)
		G. Multi region HA
		H. Bigtable is recommended for high-performance analytical workloads over 1 TB  ( eg 500GB = firestore)
		I. SSD (solid state) vs HDD (hard disk) both write can be 10000 requests per sec
			i. SSD is better than HDD
			ii. More expensive
			iii. If >10TB needa chooseHDD
			iv. HDD = not frequently, good for batch
		J. Hbase API to migrate from hadoop to here!!
		K. Google recommend storage utilization per node < 60%
		L. Even though Cloud Bigtable tablet data is stored in Google Colossus, a cluster needs to be sized appropriately so that nodes have enough resources to process the total storage in use. When instance storage utilization reaches 70% per node, additional nodes should be added.
		M. If one node down, Nothing, as the storage is separated from the node compute. (node stores pointer)
		N. BigTable is not good for ad-hoc access 
		O. Row Key
			i. Unique identifier for a row of data
			ii. No secondary index recommended in BigTable.
			iii. Big table does not support join
			iv. Row key determine node and Sstable where data is written. Ideally reads and wrties distributed evenly
			v. Avoid linearly incrementing row kets and low cardinary keys
			vi. -= Each table has one index based only
			vii. Columnds could be grouped into family (city, address, frequenetly used tgt) - up to 100 fam
			viii. All operations are atomic at the row level
			ix. Empty columns do not take any space
			x. Limit = 1000 tables per instance
			xi. Small tables are problematic. Store data in one instead of many. 1 TB!
			xii. If for time series, should consider using TALL table 
		P. Cloud Bigtable is eventually consistent. To guarantee strong consistency you must limit queries to a single cluster in an instance by using an application profile. (One master, one app profile)
		Q. To isolate batch analytics jobs from other operations in Bigtable, Google Cloud recommends using two clusters in a single instance and using app profile to route operations to the appropriate cluster.
		R. C –> Adding more nodes to a cluster (not replication) can improve the write performance https://cloud.google.com/bigtable/docs/performance 
		S. Cloud Biquery is better in terms of low latency writes. Bigtable good at consistency->Message application/
		
	5. Redis: MemoryStore
		A. Redis 
			i. Connect from K8s
			ii. Scale as needed
			iii. Up to 30GBs and 12GPS network
			iv.  Tiers 
				1) Basic - Cache with no rpelication. No cross zone. No autmatic failover
				2) Standard - Cache with replication, cross zone, automatic failover
			v. Worse than OS redis. 
				1) Cannot persist to disk
				2) Some redis commands are blocked
		B. Memcached - Tiers
			i. Cluster
			ii. Number of nodes, vcpu, memory 
			iii. Maximum of 20 nodes
			iv. 1-32cpus. 1GB t 256GB
			v. UP to 5TB in an instance/cluster
		C. Comparison
			Memcache - good for large and if need HA cluster, NO sorted set
			Redis - can support more types of data,  hyperloglog for fast but less accurate number
	
	6. ETL: Cloud composer, DataFusion
		A. ETL/ELT dev+ Batch
		B. Cloud Composer
			i. Based on Apache Airflow + K8s + Redis
			ii. Collections of tasks -> using Directed Acyclic Graph
			iii. Scriptws are stored in Cloud Storage
			iv. Python programs
			v. Logs are in web interfaec or Cloud Storage. Streaming logs available in Logs Viewer
			vi. Daily barch operation too
		C. Data Fusion
			i. Code free ETL/ELT dev
			ii. Drag and drop basically
			iii. Fusion deployed as an instance
			iv. Tiers
				1) Basic 
					a) Visual designer
					b) Transformations, SDK
				2) Enterprise
					a) Basic plus  streaming pipeline, HA, Triggers
	7. Warehouse: BigQuery, AI Model, 
		A. Types
			i. Numeric (9 decimals), Integer, Date, Datetime, Time(Clock), TimeStamp, Boolean, String, Bytes,
			ii. TIMESTAMP = absolute time 
			iii. Array (array of arrays are not allowed) Array<String>
			iv. IN Memoory Analytics BI Engine (cached)
			v. Partition maximum = 4000, Timestamp, Data, integer range or ingestion time (not string)
			vi. Does not support ProtoBuf format
			vii. External source (without import) cannot be nosql. Can be google drive !!
			viii. Columnar
			ix. Cant do Excel file
			x. Set the BigQuery dataset to be multi-regional. In the event of an emergency, use a point-in-time snapshot to recover the data.
			xi. Update is slowl; Develop a data pipeline where status updates are appended to BigQuery instead of updated. 
			xii. Data Studio cache data for an hour
			xiii. Big query not matching byte to bte if the CSV data loaded in BigQuery is not using BigQuery’s default encoding.
			xiv. Create an authorized view on the BigQuery table to control data access, and provide third-party companies with access to that view.!!!!  Dataset safer than table
			xv. BigQuery allows us to perform GeoSpatial analysis.
		B. Access Control
			i. Data access - Table/datasets/jobs/models/ reservations/ saved queries
			ii. IAM Roles - Bigquery admin |data owner, editor, viewer | Resource Admin/ Editor/Viewer ( manage resource but without financial responsibility), User
			iii. Column level Security - restrict access to sensitive info. USE IAM roles
		C. Partitioned Tables
			i. Tables divided into segments called partitions
			ii. Partition by Ingestion time (loads data into daily, date-based partitions)
			iii. Date/Timestamp partitioning (better than sharding)
			iv. Special partitions (_NULL__; UNPARTITIONED__ when values in column outside range, _PARTITIONTIME= fr tables partitioned on ingestion )
			TABLE_DATE_RANGE([myproject-1234:mydata.people],
			                    TIMESTAMP('2014-03-25'),
			                    TIMESTAMP('2014-03-27'))
			v. Integer Range partition
			vi. Requiring partition filter.
			vii. devicegroup#serialnumber#timestamp ( where serialnumber  and timestamp = PM)
		D. Clustered Tables (basically family in bigtable)
			i. Data sorted based on values in one or more column
			ii. Can improve performance of aggregate queries
			iii. Reduce scanning
			iv. When table is partitioned (cannot do if not)
			v. Autonomatically reclusters in the background
		E. Load
			i. Cloud Storage/ DML bulk loads/ Big Query IO Transform in Dataflow
			ii. Arvo > JSON! > CSV
		F. Warehouse
			i. For analysis
			ii. Transfer tables
			iii. Transfer groups of tables - with BigQuery Data Transfer Service
			iv. Data Studio/ Looker - > reports
			v. Data governance - 3 components - people, processes for control, 
			vi. Data Discovery Origin, provenance, owner, quality, data catalog
			vii. Migration = transfer table one by one. Do not modify table in the process
			viii. Flat rate model/ = fixed price but has limit. Can set up priority model
		G. Code
			i. Approx count (estimated count but much faster)
			ii. Use the ROW_NUMBER window function with PARTITION by unique ID along with WHERE row equals 
		H. AI
			i. Introduction - Unsupervised (clustering), Supervised, Reinforcement
			ii. Approaches - Symbolic (Decision trees, Random Forest, Naïve Bayes, knearest neighbor, support vector machine, regression (bias and variance)) and Machine learning
			iii. Features - Bucket (number ranged values->medium, high), One hot encoding (variable->number)
			iv. Softmax for multiclassification (
			v. \
			vi. 0-1) , Tanh (-1,1) ->two class, relu= regression
			vii.  Accuracy is used for Classification problems not for Regression problems.
			viii. F-Score = for classification, precision and recall. RMSE, RSE = for regression metric
			ix. Classification problems all matrix are fine
			x. BigQuery ML does not allow for hyperparameter tuning
			xi. Auto ML NLP- Ensure you provide at minimum 10 training documents per label, but ideally 100 times more documents for the most common label than for the least common label.
			
			xii. Machinelearning - converts null values to 0!
			Solve overfitting
			Feature selection = L1 Regularization = Lasso (median). For elinating not related, weigh heavily on one!
			Ridge Regression = l2 (Use smaller set of features) (mean) (for cha ng dor values)
			Dropout = regularization too
			Data augmentation to reduce overfitting
			Increase Validation set 
			Increase data
			Use a smaller set of features (not featurecross)
			Solve underfitting
			Feature Cross (Cartesian product of 2 or more features) (to solve underfitting),  increasing model complexity generally helps when you have an underfitting problem helps underfitting
			
			xiii. External 
				1) BigQuery can access data in external sources, known as federated sources. Instead of first loading data into BigQuery, you can create a reference to an external source. External sources can be Cloud Bigtable, Cloud Storage, and Google Drive. When accessing external data, you can create either permanent or temporary external tables. Permanent tables are those that are created in a dataset and linked to an external source. Dataset-level access controls can be applied to these tables. When you are using a temporary table, a table is created in a special dataset and will be available for approxi- mately 24 hours. Temporary tables are useful for one-time operations, such as loading data into a data warehouse.
			xiv. Models
				1) Serverless GCP option
				2) Train and deploy model
				3) Services
					a) Cloud AutoML - design for model builders with limited ML experience, GUI
					b) AI Platform Training - Tensorflow, Scikit Learn, XGBoost
					c) Kubeflow - for k8s. Packages models like applications
					d) Dataproc with Spark ML - Spark ML
					e) Big Query ML - serverless analytical database; SQL
				4) Prebuilt = Vision (images), Video, Natural Language, Conversation
				5) Deploy = AI Platform (Serverless), Dataproc, Self managed
				6) GPU vs TPU
					a) Attach them!
					b) In order to make sure it can work, install GPU driver, grant owner basic role to 
					c)  Graphic-> matrix multiplication, expensive, for small and medium
					d) Use Deep learning virtual image
					e)  TPU (tensor) -> Large dataset + cheaper
				7) Monitor
					a) Monitor for data skew
					b) Watch for changes independencies
					c) Models are refreshed as needed
					d) Assess model prediction quality
			xv. Big Query
				1) Types 
					a) Linear regression
					b) Binary and multiclass logistic regression
					c) K means clustering
					d) Time series forecasr
					e) Matrix factorization
					f) Boosted tree and xgboost
					g) Tensorflow (imported)
					h) Automl tables(Automatically use model for you)
				2) Code 
					a) Create model ''?" OPTIONS (model_type=''")
					b) Model type, input label, regularization, learning rate, early stop
					c) SELECT FROM ML.Predict(MODEL, 'model_name')
					d) SELECT * FROM ML.EVALUATE(MODEL, 'model_name')
					e) 
					f) BQ query with --dry-run - estimate cost
			xvi. 
	8. Distribute:Pubsub
		A. Message queue service (kafka)
		B. MAX SIZZE 10 MB!!!
		C. Resource Location Restriction organization policy
		D. Unackqoledged = 7days retention (kafka = forever)
		E. One app writes and another reads. Asynchronously
		F. Topic + Subscriptions
		G. Command = gcloud functions deploy (function runs when a message comes) 
		H. At least one; not at most one. Just like Kafka
		I. If message wants schema, define it in topic creation
		J. Supports Arvo (!!!! This one recommmended), Protocol Buffer schema
		K. Duplicate messages in the topic could be because of delayed acknowledgements. / 
		L. Should use timestamp
		M. Recommend to use Service acc for authorization
		N. Undelivered message
			i. Your Dataflow subscriber is unable to keep up with the rate of incoming messages.
			ii. The subscriber is not acknowledging messages as they are pulled.
		O. Cloud Function is more efficient than App engine for this
		P. Rollback = Snapshot!!
	9. Streaming: Dataflow, DataProc 
		A. Dataflow
			i. Stream and Batch Processing
			ii. Cloud Dataflow flexible resource scheduling (Flexors)
			iii. Apache Beam Runner
			iv. Horizontally scalable
			v. Cloud Dataflow flexible resource scheduling (Flexors)  - Preemptive VM (Google recommend less than <30%) (Use graceful decommissioning to keep work doing during downscale vs Cancel (the opposite) , no work right after downscale), Drain feature for code update in dataflow!!!
			vi. ETL
			vii. Sliding Window - can be sliding (repeat) or tumbling ( not repeat), no ("user" window), Session (based on the seswsion)
			viii. Watermark!!!!!! Is Dataflow's = distinguish late arrival data
			ix. Side input = add another input when things are processed
			x. Can use "pardo' to fileter in real time
				1) Add a ParDo transform in Cloud Dataflow to discard corrupt elements.
			xi. Pcollection  = key value
			xii. Set a non-global windowing function. Before setting a global one
			xiii. Metric
				1) Data_watermark_age = age of most recent item that has been processed
				2) System_lag =  maximum duration that an item has been waiting
				3) Elapsed_time = elapsed time of the pipeline
				4) Element_count = number of items
			xiv. maxNumWorkers ( not maxWorker)
			xv. A lot of APIs needa be enabled
			xvi. Support Stream processing (Dataproc not)
			xvii. Use a DirectRunner to test-run the pipeline using local compute power, and a staging storage bucket
			xviii. Implement a try-catch block that transforms the both good and bad data. Create an additional output to use a new PCollection that can be output to Pub/Sub for later analysis.
			xix. The Flatten transformation merges multiple PCollection objects (CSV) into a single logical PCollection, whereas Join transforms like CoGroupByKey attempt to merge data where there are related keys in the two datasets. 
			xx. Preemptible workers only function as processing nodes.
			
		B. DataProc
			i. Spark and hadoop service
			ii. Large scale batch processing and ml
			iii. Ephermeral - run only when it is needed
			iv. Spark Broadcasting for add other input (just as side input in dataflow)
			v. Spark SQL -> SQL/ Python
			vi. Integrate with Cloud Storage Connect!!!
				1) Cloud Storage instead of HDFS attached storage
				2) Automatically installed
				3) Interportability
				4) HA
				5) Quick setup
			vii. Code 
				1) Submit a kob = gcloud dataproc jobs submit sparl
				2)  gcloud dataproc clusters create = create
				3) Log = --driver-log-levels
				4)  --num-secondary-workers = preemptivbe VM
				5) Dataoric cannot add master node once created
				6) FetchFailedException = data lost when node is decommissioned
				7) Dataproc Workflow Template allows implementing DAG (Directed Acyclic Graphs)
				8) Does not support FLINK job
				9) --no-address  for private
		
		C. Cloud Data Catalog 
			i. includes all metadata from source, 
			ii. not support Datastore and DataFusion
			iii. Based on Cloud Spanner
		
		D. Prefer Ephemeral cluster than contyinually running (by Google); stop running when no need + Preemptive nodes
		E. Create custom Dataproc image that fulfils the customer requirements and use it to deploy a Dataproc cluster.
		F. One subscription  = for different jobs
		G. Rmb dataproc /data flow must be gcloud dataflow sql query (dataflow instead of bigquery)
		H. Dataproc!= serverless
	10. MultiCloud: Anthos, Cloud Composer (!)
	11. Consideration:
		A. Consistency
			i. Strict - any writes are immediately available for all processes - strong
			ii. Eventual - processes may read older version - weak
		B. Logging
			i. Cloud Logging
				1) Retains logs 30 days!!!
				2) Stream logs to Pub Sub and other third party 
				3) Can create sink to other third party/database
				4) If want inexpensive store -> store in Cloud object storage
			ii. Alert
			iii. Install monitoring agent on VM (operations aka stackdriver) (Stackdriver audit log logs who and what did that)( The Stackdriver Logging agent requires the fluentd!!!! ->SQL plugin to be configured to read logs from your database application.)
			iv. Cloud Monitor uptime check and if the uptime check fails send a notification to you. 
			
		C. Security
			i. Accounts
				1) Google - developer, admin
				2) Cloud Identity - member of an organization (GSUITE)
				3) Google group 
				4) Service account - an associate with an instance or a user (for application based)
				5) Access control = Resource, Permission, Roles
			ii. Resource Hierachy
				1) Organization -> Folder -> Project -> Resource
			iii. Roles
				1) Predefined
				2) Custom
				3) Primitive - Owner, Editor, Viewer
				4) Should use predefined. Designed with previous privilledge
			iv. Data Loss Prevention API - Financial info
				1) Discover and classify data
					a) Country specific identifiers
				2) Automatically mask data
				3) Measure re-identification task
			v. Infotypes
			Pattern detectors to identitfy sensitive information
			
			vi. Legal Compliance
				1) HIPAA - Health Insurance Portability and Accountability Act
					a) HIPAA Privacy Rule
					b) HIPAA Security Rule
				2) Health Information Technology for Economics and Clinical Health (HITECH)
					a) Covers trasmission of Data
				3) Children's Online Privacy Protection Act 
					a) Posting clear pricacy policy
					b) Direct notice to parents before collecting info
				4) General Data Protection Regulation (EU) GDPR
				5) PCI Data security (payment card industry)
		D. Encryption
			i. Encryption at rest is used when data is persisted AES256/128
			ii. Encryption in motion = when transmitting data on network 
				1) Internal google network = ALTS (application layer transport security)
				2) Public internet = TLS
				
		E. Google Managed Keys
			i. Google managed key
				1) Default key management method
			ii. Customer managed key
				1) Don’t want to support the whole key management infra. Maybe just application level
			iii. Customer supplied encruption keys
				1) Complete control over the key. Keep key on premise
			
			
			
			5 practice exams on Udemy
			
			Sample Questions
			https://docs.google.com/forms/d/e/1FAIpQLSfkWEzBCP0wQ09ZuFm7G2_4qtkYbfmk_0getojdnPdCYmq37Q/viewform
			
			
			
			Extra:
			
			Wide models are used for memorization. Deep models are for generalization.
			Deep and wide models are ideal for a recommendation application.
			
			ORC file format for HIVE/HDFS
			
			https://cloud.google.com/pubsub/docs/create-topic-gcloud
			


			Order tables appropriately in the query, with the larger table on the left side of the JOIN and the smaller table of the right side of the JOIN.
			
			
			An alert based on an increase of subscription/num_undelivered_messages for the source and a rate of change decrease of instance/storage/ used_bytes for the destination
			
			Big query vs hive = big query is columnar while hive isnt
			Cloud pub sub vs kafka = pubsub subscriber can also push
			MariaDB needs costume metrics , and stackdriver built-in monitoring tools will not provide these metrics . Opencensus Agent will do this for you 
			
			Exponential backoff strategy = error handling strategy for network applications in which a client periodically retries a failed request with increasing delays between requests.
			
			 associated data in ARRAYs, since the associated data are all of the same time.
			 Linear Regression provides interpretability 
			
			Use Cloud Speech-to-Text API, and send requests in a synchronous mode.
			Use  more training instances. => cab solve precision problem
			BigQuery (not Big table ) connects to BI Tool
			Pig on MR is very slow as it is disk based processing. Spark can be python
			
			
			Youtube analysis can be directly used in Data Studio
			
			Normalisation = good for saving space but if only consider query, donormalisation is fastest
			AI model on distributed scaled tier 
				1. Parameter server
				2. Worker
				3. Masters
			(Custom = change the no of para server and worker ) Not master (only one)vert
			
			
			Fewest step - be careful!
		
			ProtoBuf = Query not ok; but pubsub ok 
			 Using LIMIT only with clustered tables can reduce the amount of data scanned. Using LIMIT on non-clustered tables does not limit the number of bytes scanned. 
			
			
			Batch vs online prediction AI
			- Online = response message
			- Batch handle are optimized to handle high volume 
			
			
			-------------------------------------------------------------------------------------------------------------------------------------------------------
			
			
			
		
			Logistic = Classification!!!!!
			
			
			A is correct because Google recommends using Cloud Storage instead of HDFS as it is much more cost effective especially when jobs aren’t running.
			
			
			 `bigquery-public-data.noaa_gsod.gsod*`
			AutoML NLP needs train. NLP API no need
				
			
			HA does not help failover
			
			
			Every staff should have their own data lab notebook
			
			Google Cloud ml engine only supports Tensorflow (not torch)
			
			Agree with B as the table split is region / location based C: table level access is possible in Cloud IAM, which can be used to our advatnage for individual regions.
			
			Google App Engine Cron Service  (scheduled batch work, cheap)
			
			Regional storage is cheap (Cloud storage)
			
			
			
			
			Real exam 
			
			Dofn error why
			
			cluster partition example
			
			You have a data pipeline with a Cloud Dataflow job that aggregates and writes time series metrics to Cloud Bigtable. This data feeds a dashboard used by thousands of users across the organization. You need to support additional concurrent users and reduce the amount of time required to write the data. Which two actions should you take? (Choose two.)
			• B. Increase the maximum number of Cloud Dataflow workers by setting maxNumWorkers in PipelineOptions Most Voted
			• C. Increase the number of nodes in the Cloud Bigtable cluster Most Voted
			
			
			https://www.examtopics.com/exams/google/professional-data-engineer/view/
			
Google Cloud Data Engineer Certification Sample Exam Questions